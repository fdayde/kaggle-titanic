{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggle Titanic: 2 - Data Analysis\n",
    "\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from scipy.stats import ks_2samp, gaussian_kde\n",
    "\n",
    "import missingno as msno\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (accuracy_score, confusion_matrix,\n",
    "                             classification_report, roc_auc_score,\n",
    "                             roc_curve)\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Add the \"src\" directory to the system path\n",
    "project_root  = os.path.abspath('../')\n",
    "\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "from src.utils import EnvironmentInfo, PathManager\n",
    "from src.project_specific import create_data_dictionnary_df, DataOverview, run_dm_pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working Environment\n",
    "\n",
    "### Variables & Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local.\n",
      "Dataset directory: c:\\Users\\Florent\\Documents\\data_science\\kaggle-titanic\\data\n",
      "Working directory: c:\\Users\\Florent\\Documents\\data_science\\kaggle-titanic\\data\\working\n",
      "\n",
      "Dataset Path: c:\\Users\\Florent\\Documents\\data_science\\kaggle-titanic\\data\n",
      "Working Path: c:\\Users\\Florent\\Documents\\data_science\\kaggle-titanic\\data\\working\n",
      "\n",
      "DATASET_PATH: c:\\Users\\Florent\\Documents\\data_science\\kaggle-titanic\\data\n",
      "WORKING_PATH: c:\\Users\\Florent\\Documents\\data_science\\kaggle-titanic\\data\\working\n"
     ]
    }
   ],
   "source": [
    "path_manager = PathManager(dataset_name=\"titanic\")\n",
    "print(\"\\n\" + str(path_manager))\n",
    "\n",
    "DATASET_PATH = path_manager.dataset_path\n",
    "WORKING_PATH = path_manager.working_path\n",
    "\n",
    "print(f\"\\nDATASET_PATH: {DATASET_PATH}\")\n",
    "print(f\"WORKING_PATH: {WORKING_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test dataset memory usage: 0.13 MB\n",
      "Train dataset memory usage: 0.28 MB\n"
     ]
    }
   ],
   "source": [
    "test = pd.read_csv(os.path.join(DATASET_PATH, \"test.csv\"))\n",
    "train = pd.read_csv(os.path.join(DATASET_PATH, \"train.csv\"))\n",
    "\n",
    "datasets_dict = {\n",
    "    \"train\": train,\n",
    "    \"test\": test\n",
    "}\n",
    "\n",
    "test_memory = test.memory_usage(deep=True).sum()\n",
    "train_memory = train.memory_usage(deep=True).sum()\n",
    "print(f\"Test dataset memory usage: {test_memory / (1024**2):.2f} MB\")\n",
    "print(f\"Train dataset memory usage: {train_memory / (1024**2):.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target Variable: Survived\n",
      "Target Variable 'Survived' not found in DataFrame test.\n",
      "'Embarked' is S for passenger at index 152. Imputing 'Fare' with median fare 8.05 for 'Pclass' 3 and 'Embarked' S.\n",
      "Remaining missing 'Fare' values in test dataset: 0\n",
      "Remaining missing 'Embarked' values in train dataset: 0\n",
      "Remaining missing 'Age' values in the combined dataset: 0\n",
      "\n",
      "Categorical_features: ['Pclass', 'Sex', 'Embarked', 'Simplified_Title', 'Deck', 'Is_Alone']\n",
      "Numerical features: ['PassengerId', 'Age', 'Fare', 'Family_Size']\n",
      "target: Survived\n"
     ]
    }
   ],
   "source": [
    "result = run_dm_pipeline(test, train)\n",
    "\n",
    "test_processed = result[\"test\"]\n",
    "train_processed = result[\"train\"]\n",
    "categorical_features = result[\"categorical_features\"]\n",
    "numerical_features = result[\"numerical_features\"]\n",
    "target = result[\"target\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bivariate Analysis vs Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bivariate Analysis\n",
    "print(\"\\n=== Bivariate Analysis ===\")\n",
    "\n",
    "# Relationship between Categorical Features and Target\n",
    "for feature in categorical_features:\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    sns.countplot(data=train, x=feature, hue=target, order=train[feature].value_counts().index)\n",
    "    plt.title(f'{feature} vs {target}')\n",
    "    plt.xlabel(feature)\n",
    "    plt.ylabel('Count')\n",
    "    plt.legend(title=target)\n",
    "    plt.show()\n",
    "\n",
    "# Relationship between Numerical Features and Target\n",
    "for feature in numerical_features:\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    sns.boxplot(x=target, y=feature, data=train)\n",
    "    plt.title(f'{feature} vs {target}')\n",
    "    plt.xlabel(target)\n",
    "    plt.ylabel(feature)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Outlier Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outlier Detection\n",
    "print(\"\\n=== Outlier Detection ===\")\n",
    "for feature in numerical_features:\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    sns.boxplot(x=train[feature])\n",
    "    plt.title(f'Boxplot of {feature}')\n",
    "    plt.xlabel(feature)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlation Analysis\n",
    "##### Numerical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation Analysis\n",
    "print(\"\\n=== Correlation Analysis ===\")\n",
    "# corr = train.select_dtypes(include=[np.number]).corr()\n",
    "corr = train[numerical_features].corr()\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(corr, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "pclass_sex_ct = pd.crosstab(train['Pclass'], train['Sex'], normalize='index') * 100\n",
    "chi2, p, dof, expected = chi2_contingency(pclass_sex_ct)\n",
    "print(pclass_sex_ct)\n",
    "print(\"Chi-square statistic:\", chi2)\n",
    "print(\"p-value:\", p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pclass_deck_ct = pd.crosstab(train['Pclass'], train['Deck'])\n",
    "chi2, p, dof, expected = chi2_contingency(pclass_deck_ct)\n",
    "print(pclass_deck_ct)\n",
    "\n",
    "print(\"Chi-square statistic:\", chi2)\n",
    "print(\"p-value:\", p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pclass_embarked_ct = pd.crosstab(train['Pclass'], train['Embarked'])\n",
    "chi2, p, dof, expected = chi2_contingency(pclass_embarked_ct)\n",
    "print(pclass_embarked_ct)\n",
    "\n",
    "print(\"Chi-square statistic:\", chi2)\n",
    "print(\"p-value:\", p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deck_embarked_ct = pd.crosstab(train['Deck'], train['Embarked'])\n",
    "chi2, p, dof, expected = chi2_contingency(deck_embarked_ct)\n",
    "print(deck_embarked_ct)\n",
    "\n",
    "print(\"Chi-square statistic:\", chi2)\n",
    "print(\"p-value:\", p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- It seems that 3rd class passengers are more likely to embark from Southampton, and 1st class passengers are more likely to embark from Cherbourg.\n",
    "- there is a link between Pclass and Deck but there might be too many categories for a chi2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### One hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encoding\n",
    "train_df = pd.get_dummies(train, columns=['Pclass', 'Sex', 'Embarked', 'Simplified_Title', 'Deck'], drop_first=True)\n",
    "test_df = pd.get_dummies(test, columns=['Pclass', 'Sex', 'Embarked', 'Simplified_Title', 'Deck'], drop_first=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Correlations between the levels of the categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert categorical variables to dummy/one-hot encoded variables\n",
    "encoded_df = pd.get_dummies(train[['Pclass', 'Sex', 'Embarked', 'Simplified_Title', 'Deck']], drop_first=True)\n",
    "\n",
    "# Calculate the correlation matrix on the encoded data\n",
    "correlation_matrix = encoded_df.corr()\n",
    "\n",
    "# Display the correlation matrix\n",
    "print(correlation_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the correlation matrix for correlations with an absolute value of at least 0.6\n",
    "filtered_corr_matrix = correlation_matrix[(correlation_matrix.abs() >= 0.6)]\n",
    "\n",
    "# Drop rows and columns that are entirely NaN (where all correlations were less than 0.6)\n",
    "filtered_corr_matrix.dropna(how='all', axis=0, inplace=True)\n",
    "filtered_corr_matrix.dropna(how='all', axis=1, inplace=True)\n",
    "\n",
    "# Print the filtered correlation matrix\n",
    "print(filtered_corr_matrix)\n",
    "\n",
    "# Optional: Visualize the filtered correlation matrix with a heatmap\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(filtered_corr_matrix, annot=True, cmap='coolwarm', fmt='.2f')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train vs Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare summary statistics of numerical features\n",
    "numerical_features = ['Age', 'Fare', 'Parch', 'SibSp']\n",
    "\n",
    "train_summary = train[numerical_features].describe()\n",
    "test_summary = test[numerical_features].describe()\n",
    "\n",
    "print(\"Train Set Summary Statistics:\")\n",
    "print(train_summary)\n",
    "print(\"\\nTest Set Summary Statistics:\")\n",
    "print(test_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ks_2samp\n",
    "\n",
    "# Perform Kolmogorov-Smirnov test for numerical features\n",
    "for feature in numerical_features:\n",
    "    ks_stat, p_value = ks_2samp(train[feature].dropna(), test[feature].dropna())\n",
    "    print(f\"{feature} - KS Statistic: {ks_stat}, P-value: {p_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to perform Chi-Square test for a categorical feature\n",
    "def chi_square_test(train, test, feature):\n",
    "    train_counts = train[feature].value_counts()\n",
    "    test_counts = test[feature].value_counts()\n",
    "\n",
    "    # Combine into a contingency table\n",
    "    contingency_table = pd.DataFrame([train_counts, test_counts], index=['Train', 'Test']).fillna(0)\n",
    "\n",
    "    # Perform the Chi-Square test\n",
    "    chi2, p, dof, expected = chi2_contingency(contingency_table)\n",
    "\n",
    "    return chi2, p\n",
    "\n",
    "# Perform Chi-Square tests for each categorical feature\n",
    "for feature in categorical_features:\n",
    "    chi2, p = chi_square_test(train, test, feature)\n",
    "    print(f\"{feature} - Chi-Square: {chi2}, P-value: {p}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Pclass and Embarked distributions are not the same between Test and Train."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Datasets for analysis\n",
    "\n",
    "- numerical features:  ['PassengerId', 'Age', 'SibSp', 'Parch', 'Fare'] \n",
    "- categorical features:  ['Pclass', 'Simplified_Title', 'Sex', 'Deck', 'Embarked'] \n",
    "- target:  Survived"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train[numerical_features + categorical_features + ['Survived']]\n",
    "test_df = test[numerical_features + categorical_features]\n",
    "\n",
    "print(train_df.head())\n",
    "print(test_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### One hot encoding\n",
    "\n",
    "We can choose the most represented category as the level to drop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train.Pclass.value_counts())\n",
    "print(train.Embarked.value_counts())\n",
    "print(train.Sex.value_counts())\n",
    "print(train.Simplified_Title.value_counts())\n",
    "print(train.Deck.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_enc_choose_cat(df):\n",
    "    \"\"\" \n",
    "    Function to one hot encode categorical features and drop the most represented category.\n",
    "    \"\"\"\n",
    "    encoded_df = pd.get_dummies(df, columns=['Pclass', 'Sex', 'Embarked', 'Simplified_Title', 'Deck'], drop_first=False)\n",
    "    encoded_df = encoded_df.drop(columns=['Pclass_3', 'Sex_male', 'Simplified_Title_Commoner', 'Deck_Unknown', 'Embarked_S'])\n",
    "    return encoded_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_train_df = one_hot_enc_choose_cat(train_df)\n",
    "encoded_test_df = one_hot_enc_choose_cat(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyses\n",
    "\n",
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing\n",
    "\n",
    "- Missing values\n",
    "- Encoding categorical features\n",
    "- Scaling numerical features\n",
    "- Train-Test split for validation\n",
    "\n",
    "##### Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.loc[:, 'Age'] = train_df['Age'].fillna(train_df['Age'].median())\n",
    "test_df.loc[:, 'Age'] = test_df['Age'].fillna(test_df['Age'].median())\n",
    "\n",
    "train_df.loc[:, 'Fare'] = train_df['Fare'].fillna(train_df['Fare'].median())\n",
    "test_df.loc[:, 'Fare'] = test_df['Fare'].fillna(test_df['Fare'].median())\n",
    "\n",
    "train_df.loc[:, 'Embarked'] = train_df['Embarked'].fillna(train_df['Embarked'].mode()[0])\n",
    "test_df.loc[:, 'Embarked'] = test_df['Embarked'].fillna(test_df['Embarked'].mode()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"train:\\n\", train_df.isna().sum(), '\\n')\n",
    "print(\"test:\\n\", test_df.isna().sum(), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Encoding categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encoding\n",
    "encoded_train_df = pd.get_dummies(train_df, columns=['Pclass', 'Sex', 'Embarked', 'Simplified_Title', 'Deck'], drop_first=True)\n",
    "encoded_test_df = pd.get_dummies(test_df, columns=['Pclass', 'Sex', 'Embarked', 'Simplified_Title', 'Deck'], drop_first=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  Scaling numerical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "encoded_train_df[numerical_features] = scaler.fit_transform(encoded_train_df[numerical_features])\n",
    "encoded_test_df[numerical_features] = scaler.transform(encoded_test_df[numerical_features])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Train-Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = encoded_train_df.drop('Survived', axis=1)\n",
    "y_train = encoded_train_df['Survived']\n",
    "\n",
    "X_train_split, X_val_split, y_train_split, y_val_split = train_test_split(X_train, y_train, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit\n",
    "logreg = LogisticRegression(max_iter=500)\n",
    "logreg.fit(X_train_split, y_train_split)\n",
    "\n",
    "# predict\n",
    "y_pred = logreg.predict(X_val_split)\n",
    "\n",
    "# evaluate\n",
    "print(f'Accuracy: {accuracy_score(y_val_split, y_pred)}')\n",
    "\n",
    "print('Confusion Matrix:')\n",
    "print(confusion_matrix(y_val_split, y_pred))\n",
    "\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_val_split, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- C (Inverse of Regularization Strength):\n",
    "A smaller value of C applies stronger regularization, while a larger value reduces regularization.\n",
    "Expanding the range to [0.01, 0.1, 1, 10, 100, 1000] helps to explore more potential options for tuning regularization strength.\n",
    "\n",
    "- penalty:\n",
    "'l1' (Lasso) and 'l2' (Ridge) are commonly used penalties. By adding 'elasticnet', you can combine both L1 and L2 regularization (note: elasticnet requires the solver='saga').\n",
    "'none' can be tested for a logistic regression without regularization.\n",
    "\n",
    "- solver:\n",
    "'liblinear': Good for small datasets and supports L1 and L2 penalties.\n",
    "'saga': Supports both L1, L2, and elastic-net penalties, and is well-suited for large datasets.\n",
    "'lbfgs': Supports L2 and is good for handling large datasets and multinomial classification.\n",
    "'newton-cg': Works similarly to lbfgs and supports only the L2 penalty.\n",
    "\n",
    "- class_weight:\n",
    "Adding 'balanced' helps handle class imbalance by adjusting weights inversely proportional to class frequencies.\n",
    "\n",
    "- max_iter:\n",
    "Sometimes logistic regression models fail to converge with lower iteration limits. Testing a broader range of values ensures that your model is given enough iterations to converge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'C': [10**(i) for i in range(-4, 3)],\n",
    "    'penalty': ['l1', 'l2'],\n",
    "    'solver': ['liblinear', 'saga'],  # Added compatible solvers\n",
    "    'max_iter': [5000, 10000, 50000]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(LogisticRegression(max_iter=50000), param_grid, cv=5, verbose=1, error_score='raise')\n",
    "grid_search.fit(X_train_split, y_train_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Best Parameters: {grid_search.best_params_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_final = LogisticRegression(max_iter=5000, C=10, penalty='l2', solver='liblinear')\n",
    "logreg_final.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred_final = logreg_final.predict(X_val_split)\n",
    "\n",
    "# evaluate\n",
    "print(f'Accuracy: {accuracy_score(y_val_split, y_pred_final)}')\n",
    "\n",
    "print('Confusion Matrix:')\n",
    "print(confusion_matrix(y_val_split, y_pred_final))\n",
    "\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_val_split, y_pred_final))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpretability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coefficients \n",
    "coefficients = logreg_final.coef_[0]  \n",
    "\n",
    "# Pair the feature names with their corresponding coefficients\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': X_train.columns,\n",
    "    'Coefficient': coefficients\n",
    "})\n",
    "\n",
    "# Sort by the absolute value of the coefficient (optional)\n",
    "feature_importance = feature_importance.sort_values(by='Coefficient', ascending=False)\n",
    "\n",
    "print(feature_importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the feature importance using a bar plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='Coefficient', y='Feature', data=feature_importance)\n",
    "plt.title('Logistic Regression Feature Importance (Coefficients)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "P-values  \n",
    "The pvalues are not provided with sklearn, we would need to use statsmodels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all boolean columns to integers (0, 1)\n",
    "X_train = X_train.astype(int)\n",
    "y_train = y_train.astype(int)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# Create a DataFrame for X_train_scaled with the original column names\n",
    "X_train_scaled_df = pd.DataFrame(X_train_scaled, columns=X_train.columns)\n",
    "\n",
    "# Add a constant for the intercept\n",
    "X_train_scaled_df = sm.add_constant(X_train_scaled_df)\n",
    "\n",
    "# Fit the logistic regression model using the DataFrame with named columns\n",
    "logit_model = sm.Logit(y_train, X_train_scaled_df)\n",
    "result = logit_model.fit(method='bfgs', maxiter=50000)\n",
    "\n",
    "# Print the summary (includes p-values, coefficients, etc.)\n",
    "print(result.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f1 score : 82%  \n",
    "variables qui influent significativement sur la survie : Age, Pclass_3, Sexe_male, Title, SibSp, Parch.  \n",
    "variables significatives sont differentes des feature importantes pour la classification.\n",
    "car la reg logistique capture bien les relations linéaires, les effes d'une variables qd les autres sont constantes, mais pas les relations complexes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "# Apply PCA to reduce the features to 2 dimensions\n",
    "pca = PCA(n_components=2)\n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "X_val_split_pca = pca.transform(X_val_split)\n",
    "\n",
    "# Get the percentage of variance explained by each component\n",
    "explained_variance = pca.explained_variance_ratio_ * 100\n",
    "\n",
    "# Train the logistic regression model on PCA-transformed data\n",
    "logreg_final_pca = LogisticRegression(max_iter=5000, C=10, penalty='l2', solver='liblinear')\n",
    "logreg_final_pca.fit(X_train_pca, y_train)\n",
    "\n",
    "# Generate mesh grid for PCA space\n",
    "x_min, x_max = X_train_pca[:, 0].min() - 1, X_train_pca[:, 0].max() + 1\n",
    "y_min, y_max = X_train_pca[:, 1].min() - 1, X_train_pca[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1),\n",
    "                     np.arange(y_min, y_max, 0.1))\n",
    "\n",
    "# Predict on the grid\n",
    "Z = logreg_final_pca.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "# Create contour plot for decision boundary\n",
    "contour = go.Contour(\n",
    "    z=Z,\n",
    "    x=np.arange(x_min, x_max, 0.1),\n",
    "    y=np.arange(y_min, y_max, 0.1),\n",
    "    colorscale='Viridis',\n",
    "    opacity=0.7,\n",
    "    showscale=False,\n",
    ")\n",
    "\n",
    "# Plot the training points in PCA space\n",
    "scatter = go.Scatter(\n",
    "    x=X_train_pca[:, 0],\n",
    "    y=X_train_pca[:, 1],\n",
    "    mode='markers',\n",
    "    marker=dict(\n",
    "        color=y_train,\n",
    "        colorscale='Portland',\n",
    "        line=dict(width=1, color='black'),\n",
    "        size=8\n",
    "    ),\n",
    "    text=['Survived: {}'.format(s) for s in y_train]\n",
    ")\n",
    "\n",
    "# Set layout for the plot, including percentage of explained variance in axis labels\n",
    "layout = go.Layout(\n",
    "    title=\"Logistic Regression Decision Boundary (PCA Projection)\",\n",
    "    xaxis=dict(title=f\"PCA Component 1 ({explained_variance[0]:.2f}% variance explained)\"),\n",
    "    yaxis=dict(title=f\"PCA Component 2 ({explained_variance[1]:.2f}% variance explained)\"),\n",
    "    hovermode='closest',\n",
    "    showlegend=False\n",
    ")\n",
    "\n",
    "# Combine contour and scatter plots\n",
    "fig = go.Figure(data=[contour, scatter], layout=layout)\n",
    "\n",
    "# Show the plot\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest\n",
    "\n",
    "Preprocessing\n",
    "\n",
    "- Missing values : already dealt with\n",
    "- One hot encoding\n",
    "- feature scaling : not necessary\n",
    "- Train-Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode categorical features for train and test sets\n",
    "encoded_train_df = one_hot_enc_choose_cat(train_df)\n",
    "encoded_test_df = one_hot_enc_choose_cat(test_df)\n",
    "\n",
    "# Split the target and features in the training data\n",
    "X_train = encoded_train_df.drop(columns=['Survived'])\n",
    "y_train = encoded_train_df['Survived']\n",
    "\n",
    "X_test = encoded_test_df\n",
    "\n",
    "# Split the training data into a train and validation set\n",
    "X_train_split, X_val_split, y_train_split, y_val_split = train_test_split(X_train, y_train, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Random Forest model\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Fit the model on the training data\n",
    "rf_model.fit(X_train_split, y_train_split)\n",
    "\n",
    "# Predict on the validation set\n",
    "y_pred = rf_model.predict(X_val_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "print(f'Accuracy: {accuracy_score(y_val_split, y_pred)}')\n",
    "print('Confusion Matrix:')\n",
    "print(confusion_matrix(y_val_split, y_pred))\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_val_split, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [10, 20, 30, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "grid_search = GridSearchCV(estimator=rf_model, param_grid=param_grid, cv=5, n_jobs=-1, verbose=2)\n",
    "grid_search.fit(X_train_split, y_train_split)\n",
    "\n",
    "# Get the best model\n",
    "best_rf_model = grid_search.best_estimator_\n",
    "\n",
    "# Predict on the validation set using the best model\n",
    "y_pred_best = best_rf_model.predict(X_val_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the tuned model\n",
    "print(f'Accuracy after tuning: {accuracy_score(y_val_split, y_pred_best)}')\n",
    "print('Confusion Matrix:')\n",
    "print(confusion_matrix(y_val_split, y_pred_best))\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_val_split, y_pred_best))\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(f'Best hyperparameters: {grid_search.best_params_}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpretability\n",
    "\n",
    "Feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importance\n",
    "feature_importances = best_rf_model.feature_importances_\n",
    "\n",
    "# Create a DataFrame for the feature importance\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'Feature': X_train.columns,\n",
    "    'Importance': feature_importances\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Plot the feature importance\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='Importance', y='Feature', data=feature_importance_df)\n",
    "plt.title('Feature Importance in Random Forest Model')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost\n",
    "\n",
    "Preprocessing\n",
    "\n",
    "- Missing values: already delt with\n",
    "- One hot encoding\n",
    "- featuyre scaling ?\n",
    "- Train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode categorical features for train and test sets\n",
    "encoded_train_df = one_hot_enc_choose_cat(train_df)\n",
    "encoded_test_df = one_hot_enc_choose_cat(test_df)\n",
    "\n",
    "# Split the target and features in the training data\n",
    "X_train = encoded_train_df.drop(columns=['Survived'])\n",
    "y_train = encoded_train_df['Survived']\n",
    "\n",
    "X_test = encoded_test_df\n",
    "\n",
    "# Split the training data into a train and validation set\n",
    "X_train_split, X_val_split, y_train_split, y_val_split = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# Ensure the target variable is numeric (integer)\n",
    "y_train_split = y_train_split.astype(int)\n",
    "y_val_split = y_val_split.astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the XGBoost model\n",
    "xgb_model = xgb.XGBClassifier(random_state=42)\n",
    "\n",
    "# Fit the model on the training data\n",
    "xgb_model.fit(X_train_split, y_train_split)\n",
    "\n",
    "# Predict on the validation set\n",
    "y_pred = xgb_model.predict(X_val_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "print(f'Accuracy: {accuracy_score(y_val_split, y_pred)}')\n",
    "print('Confusion Matrix:')\n",
    "print(confusion_matrix(y_val_split, y_pred))\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_val_split, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the hyperparameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [3, 6, 9],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'subsample': [0.6, 0.8, 1.0],\n",
    "    'colsample_bytree': [0.6, 0.8, 1.0]\n",
    "}\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "grid_search = GridSearchCV(estimator=xgb_model, param_grid=param_grid, cv=5, n_jobs=-1, verbose=2)\n",
    "grid_search.fit(X_train_split, y_train_split)\n",
    "\n",
    "# Get the best model\n",
    "best_xgb_model = grid_search.best_estimator_\n",
    "\n",
    "# Predict on the validation set using the best model\n",
    "y_pred_best = best_xgb_model.predict(X_val_split)\n",
    "\n",
    "# Evaluate the tuned model\n",
    "print(f'Accuracy after tuning: {accuracy_score(y_val_split, y_pred_best)}')\n",
    "print('Confusion Matrix:')\n",
    "print(confusion_matrix(y_val_split, y_pred_best))\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_val_split, y_pred_best))\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(f'Best hyperparameters: {grid_search.best_params_}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpretability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Get feature importance from the best model\n",
    "feature_importance = best_xgb_model.feature_importances_\n",
    "\n",
    "# Create a DataFrame for the feature importance\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'Feature': X_train.columns,\n",
    "    'Importance': feature_importance\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Plot the feature importance\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='Importance', y='Feature', data=feature_importance_df)\n",
    "plt.title('XGBoost Feature Importance')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN\n",
    "\n",
    "Preprocessing\n",
    "\n",
    "- Missing values: already delt with\n",
    "- One hot encoding\n",
    "- scaling\n",
    "- Train-test split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode categorical features using your existing function\n",
    "encoded_train_df = one_hot_enc_choose_cat(train_df)\n",
    "encoded_test_df = one_hot_enc_choose_cat(test_df)\n",
    "\n",
    "# Scaling the features\n",
    "scaler = StandardScaler()\n",
    "encoded_train_df[numerical_features] = scaler.fit_transform(encoded_train_df[numerical_features])\n",
    "encoded_test_df[numerical_features] = scaler.transform(encoded_test_df[numerical_features])\n",
    "\n",
    "\n",
    "# Split the target and features in the training data\n",
    "X_train = encoded_train_df.drop(columns=['Survived'])\n",
    "y_train = encoded_train_df['Survived'].astype(int)  # Ensure the target is an integer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the training data into a train and validation set\n",
    "X_train_split, X_val_split, y_train_split, y_val_split = train_test_split(X_train_scaled, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the KNN model\n",
    "knn_model = KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "# Fit the model on the training data\n",
    "knn_model.fit(X_train_split, y_train_split)\n",
    "\n",
    "# Predict on the validation set\n",
    "y_pred = knn_model.predict(X_val_split)\n",
    "\n",
    "# Evaluate the model\n",
    "print(f'Accuracy: {accuracy_score(y_val_split, y_pred)}')\n",
    "print('Confusion Matrix:')\n",
    "print(confusion_matrix(y_val_split, y_pred))\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_val_split, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'n_neighbors': [3, 5, 7, 9, 11, 13],\n",
    "    'weights': ['uniform', 'distance'],  # Consider uniform or distance-based weighting\n",
    "    'metric': ['euclidean', 'manhattan']  # Different distance metrics\n",
    "}\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "grid_search = GridSearchCV(estimator=KNeighborsClassifier(), param_grid=param_grid, cv=5, n_jobs=-1, verbose=2)\n",
    "grid_search.fit(X_train_split, y_train_split)\n",
    "\n",
    "# Get the best model\n",
    "best_knn_model = grid_search.best_estimator_\n",
    "\n",
    "# Predict on the validation set using the best model\n",
    "y_pred_best = best_knn_model.predict(X_val_split)\n",
    "\n",
    "# Evaluate the tuned model\n",
    "print(f'Accuracy after tuning: {accuracy_score(y_val_split, y_pred_best)}')\n",
    "print('Confusion Matrix:')\n",
    "print(confusion_matrix(y_val_split, y_pred_best))\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_val_split, y_pred_best))\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(f'Best hyperparameters: {grid_search.best_params_}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpretability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lime\n",
    "import lime.lime_tabular\n",
    "\n",
    "# Create a LIME explainer\n",
    "explainer = lime.lime_tabular.LimeTabularExplainer(X_train_split, feature_names=X_train.columns, class_names=['Died', 'Survived'], discretize_continuous=True)\n",
    "\n",
    "# Select a single instance from the validation set\n",
    "instance = X_val_split[0].reshape(1, -1)\n",
    "\n",
    "# Generate explanation for that instance\n",
    "exp = explainer.explain_instance(instance.flatten(), best_knn_model.predict_proba)\n",
    "exp.show_in_notebook(show_all=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ameliorer le tuning, + ecrire une synthese de ce que fait chaque technique.\n",
    "\n",
    "Comparer les perf de chaque modele, et les feature importance.\n",
    "puis choisir le meilleur pour la soumission."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering ?  \n",
    "\n",
    "Pour trouver plus d'info sur les features qui separe les groupes, ou comment les groupes sont séparés."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "voir SHAP values, ROC curves, + d'hypermarameter tuning"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

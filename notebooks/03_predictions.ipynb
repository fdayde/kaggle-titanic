{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggle Titanic: 3 - Modelling and Predictions\n",
    "\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from scipy.stats import ks_2samp, gaussian_kde\n",
    "\n",
    "import missingno as msno\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (accuracy_score, confusion_matrix,\n",
    "                             classification_report, roc_auc_score,\n",
    "                             roc_curve)\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Add the \"src\" directory to the system path\n",
    "project_root  = os.path.abspath('../')\n",
    "\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "from src.utils import EnvironmentInfo, PathManager\n",
    "from src.project_specific import run_dm_pipeline, plot_categorical_features_vs_target, plot_numerical_features_vs_target, chi2_categorical_features, chi2_all_categorical, analyze_correlation, cramers_v, chi_square_test_train, extract_most_represented_levels, one_hot_encode_drop_most_populated\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working Environment\n",
    "\n",
    "### Variables & Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_manager = PathManager(dataset_name=\"titanic\")\n",
    "print(\"\\n\" + str(path_manager))\n",
    "\n",
    "DATASET_PATH = path_manager.dataset_path\n",
    "WORKING_PATH = path_manager.working_path\n",
    "\n",
    "print(f\"\\nDATASET_PATH: {DATASET_PATH}\")\n",
    "print(f\"WORKING_PATH: {WORKING_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(os.path.join(DATASET_PATH, \"test.csv\"))\n",
    "train = pd.read_csv(os.path.join(DATASET_PATH, \"train.csv\"))\n",
    "\n",
    "datasets_dict = {\n",
    "    \"train\": train,\n",
    "    \"test\": test\n",
    "}\n",
    "\n",
    "test_memory = test.memory_usage(deep=True).sum()\n",
    "train_memory = train.memory_usage(deep=True).sum()\n",
    "print(f\"Test dataset memory usage: {test_memory / (1024**2):.2f} MB\")\n",
    "print(f\"Train dataset memory usage: {train_memory / (1024**2):.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = run_dm_pipeline(test, train)\n",
    "\n",
    "categorical_features = result[\"categorical_features\"]\n",
    "numerical_features = result[\"numerical_features\"]\n",
    "target = result[\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = result[\"test\"][categorical_features + numerical_features]\n",
    "train_df = result[\"train\"][categorical_features + numerical_features + [target]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling\n",
    "\n",
    "### Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit\n",
    "logreg = LogisticRegression(max_iter=500)\n",
    "logreg.fit(X_train_split, y_train_split)\n",
    "\n",
    "# predict\n",
    "y_pred = logreg.predict(X_val_split)\n",
    "\n",
    "# evaluate\n",
    "print(f'Accuracy: {accuracy_score(y_val_split, y_pred)}')\n",
    "\n",
    "print('Confusion Matrix:')\n",
    "print(confusion_matrix(y_val_split, y_pred))\n",
    "\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_val_split, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- C (Inverse of Regularization Strength):\n",
    "A smaller value of C applies stronger regularization, while a larger value reduces regularization.\n",
    "Expanding the range to [0.01, 0.1, 1, 10, 100, 1000] helps to explore more potential options for tuning regularization strength.\n",
    "\n",
    "- penalty:\n",
    "'l1' (Lasso) and 'l2' (Ridge) are commonly used penalties. By adding 'elasticnet', you can combine both L1 and L2 regularization (note: elasticnet requires the solver='saga').\n",
    "'none' can be tested for a logistic regression without regularization.\n",
    "\n",
    "- solver:\n",
    "'liblinear': Good for small datasets and supports L1 and L2 penalties.\n",
    "'saga': Supports both L1, L2, and elastic-net penalties, and is well-suited for large datasets.\n",
    "'lbfgs': Supports L2 and is good for handling large datasets and multinomial classification.\n",
    "'newton-cg': Works similarly to lbfgs and supports only the L2 penalty.\n",
    "\n",
    "- class_weight:\n",
    "Adding 'balanced' helps handle class imbalance by adjusting weights inversely proportional to class frequencies.\n",
    "\n",
    "- max_iter:\n",
    "Sometimes logistic regression models fail to converge with lower iteration limits. Testing a broader range of values ensures that your model is given enough iterations to converge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'C': [10**(i) for i in range(-4, 3)],\n",
    "    'penalty': ['l1', 'l2'],\n",
    "    'solver': ['liblinear', 'saga'],  # Added compatible solvers\n",
    "    'max_iter': [5000, 10000, 50000]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(LogisticRegression(max_iter=50000), param_grid, cv=5, verbose=1, error_score='raise')\n",
    "grid_search.fit(X_train_split, y_train_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Best Parameters: {grid_search.best_params_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_final = LogisticRegression(max_iter=5000, C=10, penalty='l2', solver='liblinear')\n",
    "logreg_final.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred_final = logreg_final.predict(X_val_split)\n",
    "\n",
    "# evaluate\n",
    "print(f'Accuracy: {accuracy_score(y_val_split, y_pred_final)}')\n",
    "\n",
    "print('Confusion Matrix:')\n",
    "print(confusion_matrix(y_val_split, y_pred_final))\n",
    "\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_val_split, y_pred_final))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpretability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coefficients \n",
    "coefficients = logreg_final.coef_[0]  \n",
    "\n",
    "# Pair the feature names with their corresponding coefficients\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': X_train.columns,\n",
    "    'Coefficient': coefficients\n",
    "})\n",
    "\n",
    "# Sort by the absolute value of the coefficient (optional)\n",
    "feature_importance = feature_importance.sort_values(by='Coefficient', ascending=False)\n",
    "\n",
    "print(feature_importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the feature importance using a bar plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='Coefficient', y='Feature', data=feature_importance)\n",
    "plt.title('Logistic Regression Feature Importance (Coefficients)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f1 score : 82%  \n",
    "variables qui influent significativement sur la survie : Age, Pclass_3, Sexe_male, Title, SibSp, Parch.  \n",
    "variables significatives sont differentes des feature importantes pour la classification.\n",
    "car la reg logistique capture bien les relations lin√©aires, les effes d'une variables qd les autres sont constantes, mais pas les relations complexes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "# Apply PCA to reduce the features to 2 dimensions\n",
    "pca = PCA(n_components=2)\n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "X_val_split_pca = pca.transform(X_val_split)\n",
    "\n",
    "# Get the percentage of variance explained by each component\n",
    "explained_variance = pca.explained_variance_ratio_ * 100\n",
    "\n",
    "# Train the logistic regression model on PCA-transformed data\n",
    "logreg_final_pca = LogisticRegression(max_iter=5000, C=10, penalty='l2', solver='liblinear')\n",
    "logreg_final_pca.fit(X_train_pca, y_train)\n",
    "\n",
    "# Generate mesh grid for PCA space\n",
    "x_min, x_max = X_train_pca[:, 0].min() - 1, X_train_pca[:, 0].max() + 1\n",
    "y_min, y_max = X_train_pca[:, 1].min() - 1, X_train_pca[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1),\n",
    "                     np.arange(y_min, y_max, 0.1))\n",
    "\n",
    "# Predict on the grid\n",
    "Z = logreg_final_pca.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "# Create contour plot for decision boundary\n",
    "contour = go.Contour(\n",
    "    z=Z,\n",
    "    x=np.arange(x_min, x_max, 0.1),\n",
    "    y=np.arange(y_min, y_max, 0.1),\n",
    "    colorscale='Viridis',\n",
    "    opacity=0.7,\n",
    "    showscale=False,\n",
    ")\n",
    "\n",
    "# Plot the training points in PCA space\n",
    "scatter = go.Scatter(\n",
    "    x=X_train_pca[:, 0],\n",
    "    y=X_train_pca[:, 1],\n",
    "    mode='markers',\n",
    "    marker=dict(\n",
    "        color=y_train,\n",
    "        colorscale='Portland',\n",
    "        line=dict(width=1, color='black'),\n",
    "        size=8\n",
    "    ),\n",
    "    text=['Survived: {}'.format(s) for s in y_train]\n",
    ")\n",
    "\n",
    "# Set layout for the plot, including percentage of explained variance in axis labels\n",
    "layout = go.Layout(\n",
    "    title=\"Logistic Regression Decision Boundary (PCA Projection)\",\n",
    "    xaxis=dict(title=f\"PCA Component 1 ({explained_variance[0]:.2f}% variance explained)\"),\n",
    "    yaxis=dict(title=f\"PCA Component 2 ({explained_variance[1]:.2f}% variance explained)\"),\n",
    "    hovermode='closest',\n",
    "    showlegend=False\n",
    ")\n",
    "\n",
    "# Combine contour and scatter plots\n",
    "fig = go.Figure(data=[contour, scatter], layout=layout)\n",
    "\n",
    "# Show the plot\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest\n",
    "\n",
    "Preprocessing\n",
    "\n",
    "- Missing values : already dealt with\n",
    "- One hot encoding\n",
    "- feature scaling : not necessary\n",
    "- Train-Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode categorical features for train and test sets\n",
    "encoded_train_df = one_hot_enc_choose_cat(train_df)\n",
    "encoded_test_df = one_hot_enc_choose_cat(test_df)\n",
    "\n",
    "# Split the target and features in the training data\n",
    "X_train = encoded_train_df.drop(columns=['Survived'])\n",
    "y_train = encoded_train_df['Survived']\n",
    "\n",
    "X_test = encoded_test_df\n",
    "\n",
    "# Split the training data into a train and validation set\n",
    "X_train_split, X_val_split, y_train_split, y_val_split = train_test_split(X_train, y_train, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Random Forest model\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Fit the model on the training data\n",
    "rf_model.fit(X_train_split, y_train_split)\n",
    "\n",
    "# Predict on the validation set\n",
    "y_pred = rf_model.predict(X_val_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "print(f'Accuracy: {accuracy_score(y_val_split, y_pred)}')\n",
    "print('Confusion Matrix:')\n",
    "print(confusion_matrix(y_val_split, y_pred))\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_val_split, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [10, 20, 30, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "grid_search = GridSearchCV(estimator=rf_model, param_grid=param_grid, cv=5, n_jobs=-1, verbose=2)\n",
    "grid_search.fit(X_train_split, y_train_split)\n",
    "\n",
    "# Get the best model\n",
    "best_rf_model = grid_search.best_estimator_\n",
    "\n",
    "# Predict on the validation set using the best model\n",
    "y_pred_best = best_rf_model.predict(X_val_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the tuned model\n",
    "print(f'Accuracy after tuning: {accuracy_score(y_val_split, y_pred_best)}')\n",
    "print('Confusion Matrix:')\n",
    "print(confusion_matrix(y_val_split, y_pred_best))\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_val_split, y_pred_best))\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(f'Best hyperparameters: {grid_search.best_params_}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpretability\n",
    "\n",
    "Feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importance\n",
    "feature_importances = best_rf_model.feature_importances_\n",
    "\n",
    "# Create a DataFrame for the feature importance\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'Feature': X_train.columns,\n",
    "    'Importance': feature_importances\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Plot the feature importance\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='Importance', y='Feature', data=feature_importance_df)\n",
    "plt.title('Feature Importance in Random Forest Model')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost\n",
    "\n",
    "Preprocessing\n",
    "\n",
    "- Missing values: already delt with\n",
    "- One hot encoding\n",
    "- featuyre scaling ?\n",
    "- Train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode categorical features for train and test sets\n",
    "encoded_train_df = one_hot_enc_choose_cat(train_df)\n",
    "encoded_test_df = one_hot_enc_choose_cat(test_df)\n",
    "\n",
    "# Split the target and features in the training data\n",
    "X_train = encoded_train_df.drop(columns=['Survived'])\n",
    "y_train = encoded_train_df['Survived']\n",
    "\n",
    "X_test = encoded_test_df\n",
    "\n",
    "# Split the training data into a train and validation set\n",
    "X_train_split, X_val_split, y_train_split, y_val_split = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# Ensure the target variable is numeric (integer)\n",
    "y_train_split = y_train_split.astype(int)\n",
    "y_val_split = y_val_split.astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the XGBoost model\n",
    "xgb_model = xgb.XGBClassifier(random_state=42)\n",
    "\n",
    "# Fit the model on the training data\n",
    "xgb_model.fit(X_train_split, y_train_split)\n",
    "\n",
    "# Predict on the validation set\n",
    "y_pred = xgb_model.predict(X_val_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "print(f'Accuracy: {accuracy_score(y_val_split, y_pred)}')\n",
    "print('Confusion Matrix:')\n",
    "print(confusion_matrix(y_val_split, y_pred))\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_val_split, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the hyperparameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [3, 6, 9],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'subsample': [0.6, 0.8, 1.0],\n",
    "    'colsample_bytree': [0.6, 0.8, 1.0]\n",
    "}\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "grid_search = GridSearchCV(estimator=xgb_model, param_grid=param_grid, cv=5, n_jobs=-1, verbose=2)\n",
    "grid_search.fit(X_train_split, y_train_split)\n",
    "\n",
    "# Get the best model\n",
    "best_xgb_model = grid_search.best_estimator_\n",
    "\n",
    "# Predict on the validation set using the best model\n",
    "y_pred_best = best_xgb_model.predict(X_val_split)\n",
    "\n",
    "# Evaluate the tuned model\n",
    "print(f'Accuracy after tuning: {accuracy_score(y_val_split, y_pred_best)}')\n",
    "print('Confusion Matrix:')\n",
    "print(confusion_matrix(y_val_split, y_pred_best))\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_val_split, y_pred_best))\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(f'Best hyperparameters: {grid_search.best_params_}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpretability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Get feature importance from the best model\n",
    "feature_importance = best_xgb_model.feature_importances_\n",
    "\n",
    "# Create a DataFrame for the feature importance\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'Feature': X_train.columns,\n",
    "    'Importance': feature_importance\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Plot the feature importance\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='Importance', y='Feature', data=feature_importance_df)\n",
    "plt.title('XGBoost Feature Importance')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN\n",
    "\n",
    "Preprocessing\n",
    "\n",
    "- Missing values: already delt with\n",
    "- One hot encoding\n",
    "- scaling\n",
    "- Train-test split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode categorical features using your existing function\n",
    "encoded_train_df = one_hot_enc_choose_cat(train_df)\n",
    "encoded_test_df = one_hot_enc_choose_cat(test_df)\n",
    "\n",
    "# Scaling the features\n",
    "scaler = StandardScaler()\n",
    "encoded_train_df[numerical_features] = scaler.fit_transform(encoded_train_df[numerical_features])\n",
    "encoded_test_df[numerical_features] = scaler.transform(encoded_test_df[numerical_features])\n",
    "\n",
    "\n",
    "# Split the target and features in the training data\n",
    "X_train = encoded_train_df.drop(columns=['Survived'])\n",
    "y_train = encoded_train_df['Survived'].astype(int)  # Ensure the target is an integer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the training data into a train and validation set\n",
    "X_train_split, X_val_split, y_train_split, y_val_split = train_test_split(X_train_scaled, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the KNN model\n",
    "knn_model = KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "# Fit the model on the training data\n",
    "knn_model.fit(X_train_split, y_train_split)\n",
    "\n",
    "# Predict on the validation set\n",
    "y_pred = knn_model.predict(X_val_split)\n",
    "\n",
    "# Evaluate the model\n",
    "print(f'Accuracy: {accuracy_score(y_val_split, y_pred)}')\n",
    "print('Confusion Matrix:')\n",
    "print(confusion_matrix(y_val_split, y_pred))\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_val_split, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'n_neighbors': [3, 5, 7, 9, 11, 13],\n",
    "    'weights': ['uniform', 'distance'],  # Consider uniform or distance-based weighting\n",
    "    'metric': ['euclidean', 'manhattan']  # Different distance metrics\n",
    "}\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "grid_search = GridSearchCV(estimator=KNeighborsClassifier(), param_grid=param_grid, cv=5, n_jobs=-1, verbose=2)\n",
    "grid_search.fit(X_train_split, y_train_split)\n",
    "\n",
    "# Get the best model\n",
    "best_knn_model = grid_search.best_estimator_\n",
    "\n",
    "# Predict on the validation set using the best model\n",
    "y_pred_best = best_knn_model.predict(X_val_split)\n",
    "\n",
    "# Evaluate the tuned model\n",
    "print(f'Accuracy after tuning: {accuracy_score(y_val_split, y_pred_best)}')\n",
    "print('Confusion Matrix:')\n",
    "print(confusion_matrix(y_val_split, y_pred_best))\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_val_split, y_pred_best))\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(f'Best hyperparameters: {grid_search.best_params_}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpretability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lime\n",
    "import lime.lime_tabular\n",
    "\n",
    "# Create a LIME explainer\n",
    "explainer = lime.lime_tabular.LimeTabularExplainer(X_train_split, feature_names=X_train.columns, class_names=['Died', 'Survived'], discretize_continuous=True)\n",
    "\n",
    "# Select a single instance from the validation set\n",
    "instance = X_val_split[0].reshape(1, -1)\n",
    "\n",
    "# Generate explanation for that instance\n",
    "exp = explainer.explain_instance(instance.flatten(), best_knn_model.predict_proba)\n",
    "exp.show_in_notebook(show_all=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ameliorer le tuning, + ecrire une synthese de ce que fait chaque technique.\n",
    "\n",
    "Comparer les perf de chaque modele, et les feature importance.\n",
    "puis choisir le meilleur pour la soumission.\n",
    "\n",
    "voir SHAP values, ROC curves, + d'hypermarameter tuning\n",
    "\n",
    "\n",
    "\n",
    "### Clustering ?  \n",
    "\n",
    "Pour trouver plus d'info sur les features qui separe les groupes, ou comment les groupes sont s√©par√©s.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
